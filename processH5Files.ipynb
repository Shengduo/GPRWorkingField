{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import h5 reader\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "work_path = \"/home/shengduo/pylith-developer/build/debug/pylith-nonRegSlipLawWithVaryingB/examples/bar_shearwave/quad4/\"\n",
    "h5_path = work_path + \"output/fault/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class RunABatch\n",
    "class RunABatch:\n",
    "    # Constructor\n",
    "    def __init__(self, input_set, work_path, FourierTerms = 16):\n",
    "        # Initialize data paths and batch parameters\n",
    "        self.work_path = work_path\n",
    "        self.input_set = input_set\n",
    "        self.h5_path = work_path + \"output/fault/\"\n",
    "        self.FourierTerms = FourierTerms\n",
    "        \n",
    "        # Store all cases in the h5_path\n",
    "        self.existingCasesFile = self.h5_path + \"CaseList.csv\"\n",
    "        \n",
    "        # Flag for whether cases have been run\n",
    "        self.casesExcuted = False\n",
    "        \n",
    "        # Get existing cases\n",
    "        self.getExistingCasesOfInputSet()\n",
    "        \n",
    "        # Get input_set_toRun\n",
    "        self.input_set_toRun = list(set(input_set) - self.existingCases)\n",
    "        \n",
    "        # Run cases\n",
    "        self.runCases(self.input_set_toRun)\n",
    "        \n",
    "        # Get obsevations\n",
    "        self.Observations = self.getObservations(self.input_set)\n",
    "    \n",
    "    # Inline function gets [A, B] list\n",
    "    def getAAndB(self, fileName):\n",
    "        A_idx = fileName.find('A')\n",
    "        B_idx = fileName.find('B')\n",
    "        slash_idx = fileName.find('-')\n",
    "        \n",
    "        # Change this part !! Before applying to new name convention\n",
    "        A = float(fileName[A_idx + 1 : B_idx - 1])\n",
    "        B = float(fileName[B_idx + 1 : slash_idx])\n",
    "        return (A, B)\n",
    "    \n",
    "    \n",
    "    # Function get_existing_files_set\n",
    "    def getExistingCasesOfInputSet(self):\n",
    "        # Get all .h5 file names as a list\n",
    "        myPath = self.h5_path\n",
    "        onlyFiles = [f for f in listdir(myPath) if (isfile(join(myPath, f)) and f[-3 : ] == '.h5')]\n",
    "        self.existingCases = set([self.getAAndB(f) for f in onlyFiles])\n",
    "        \n",
    "    # Function runCases\n",
    "    def runCases(self, input_set):\n",
    "        shell_path = self.work_path + \"RunJobsJP.sh\"\n",
    "        shellRead = open(shell_path, 'r')\n",
    "        list_of_lines = shellRead.readlines()\n",
    "        shellRead.close()\n",
    "\n",
    "        AA = [ele[0] for ele in input_set]\n",
    "        BB = [ele[1] for ele in input_set]\n",
    "\n",
    "        list_of_lines[9] = \"AA=\" + str(tuple(AA)).replace(',', '') + \"\\n\"\n",
    "        list_of_lines[10] = \"BB=\" + str(tuple(BB)).replace(',', '') + \"\\n\"\n",
    "\n",
    "        shellWrite = open(shell_path, 'w')\n",
    "        shellWrite.writelines(list_of_lines)\n",
    "        shellWrite.close()\n",
    "\n",
    "        # Run the cases\n",
    "        !source $shell_path\n",
    "        \n",
    "        self.casesExcuted = True\n",
    "        \n",
    "        # Return from shell\n",
    "        return\n",
    "    \n",
    "    \n",
    "    # Function getObservations for the input_set\n",
    "    def getObservations(self, input_set):\n",
    "        # Initialize Observations\n",
    "        Observations = []\n",
    "        \n",
    "        # Check if the cases have been excuted\n",
    "        if self.casesExcuted == False:\n",
    "            return Observations\n",
    "        \n",
    "        # Loop through all Inputs\n",
    "        for input_ele in input_set:\n",
    "            # Open the file\n",
    "            h5_file = self.h5_path + \"A\" + str(input_ele[0]) + \"_B\" + str(input_ele[1]) + \"-fault.h5\"\n",
    "            f = h5py.File(h5_file, 'r')\n",
    "\n",
    "            # Get time\n",
    "            time = np.array(f['time']).reshape([-1])\n",
    "            time = time - np.min(time)\n",
    "            nOfTSteps = time.shape[0]\n",
    "\n",
    "            # Get Slip rates\n",
    "            SlipRates = np.array(f['vertex_fields']['slip_rate'])\n",
    "            Vx = SlipRates[:, :, 0].transpose()\n",
    "            Vy = SlipRates[:, :, 1].transpose()\n",
    "            nOfNodes = Vx.shape[0]\n",
    "\n",
    "            # Find the Fourier coefficients\n",
    "            FourierTerms = self.FourierTerms\n",
    "            T = np.max(time)\n",
    "\n",
    "            # Compute the Fourier terms\n",
    "            Ks = np.array(range(FourierTerms))\n",
    "            coskPiTt = np.cos(Ks.reshape([-1, 1]) * np.pi / T * time)\n",
    "            VxcoskPiTt = np.concatenate([coskPiTt * Vxi.reshape([1, -1]) for Vxi in Vx], 0)\n",
    "\n",
    "            # Compute the fourier coefficients\n",
    "            # print('time.shape: ', time.shape)\n",
    "            observation = np.trapz(VxcoskPiTt, x=time)\n",
    "\n",
    "            # Append the result from this file\n",
    "            Observations.append(observation)\n",
    "            \n",
    "        return Observations\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/shengduo/InverseProblems/GPRWorkingField\r\n"
     ]
    }
   ],
   "source": [
    "# Input example of getting observations\n",
    "AA = np.linspace(0.002, 0.014, 7)\n",
    "BB = AA + 0.004\n",
    "\n",
    "# Get input set\n",
    "input_set = set([(AA[i], BB[i]) for i in range(len(AA))])\n",
    "\n",
    "# Test class RunABatch\n",
    "myBatch = RunABatch(input_set, work_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.004, 0.008],\n",
       "       [0.014, 0.018],\n",
       "       [0.006, 0.01 ],\n",
       "       [0.01 , 0.014],\n",
       "       [0.002, 0.006],\n",
       "       [0.008, 0.012],\n",
       "       [0.012, 0.016]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(list(myBatch.input_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the sampling packages\n",
    "import emcee\n",
    "\n",
    "# Set up a GP\n",
    "myGP = GP_predictor(myBatch.input_set, myBatch.Observations)\n",
    "\n",
    "# Define the log probability function\n",
    "def log_prob(u, y, GP, si_eta = 1.):\n",
    "    return - 0.5 / si_eta ** 2 * np.sum((y - GP.predict(u)) ** 2, axis = 1)\n",
    "\n",
    "# Use a sample y\n",
    "y = myBatch.Observations[0]\n",
    "\n",
    "# Sample from p(u|y)\n",
    "uSt = np.array([0.002, 0.006])\n",
    "uLength = np.array([0.01, 0.01])\n",
    "\n",
    "ndim, nwalkers = 2, 100\n",
    "p0 = np.random.uniform(size = [nwalkers, ndim]) * uLength + uSt\n",
    "\n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, log_prob, args=[y, myGP])\n",
    "sampler.run_mcmc(p0, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[0.00247621 0.00651726].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [85]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmyGP\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_scaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0.00247621\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.00651726\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/preprocessing/_data.py:973\u001b[0m, in \u001b[0;36mStandardScaler.transform\u001b[0;34m(self, X, copy)\u001b[0m\n\u001b[1;32m    970\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    972\u001b[0m copy \u001b[38;5;241m=\u001b[39m copy \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy\n\u001b[0;32m--> 973\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(X):\n\u001b[1;32m    984\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_mean:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/base.py:566\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    564\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation should be done on X, y or both.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    565\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 566\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m     out \u001b[38;5;241m=\u001b[39m X\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:769\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    767\u001b[0m     \u001b[38;5;66;03m# If input is 1D raise error\u001b[39;00m\n\u001b[1;32m    768\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 769\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    770\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected 2D array, got 1D array instead:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124marray=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    771\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    772\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    773\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mif it contains a single sample.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[1;32m    774\u001b[0m         )\n\u001b[1;32m    776\u001b[0m \u001b[38;5;66;03m# make sure we actually converted to numeric:\u001b[39;00m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype_numeric \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOUSV\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[0.00247621 0.00651726].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "myGP.input_scaler.transform(np.array([0.00247621, 0.00651726]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.uniform()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Gaussian-regression related functions\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "\n",
    "# Pre-process the data\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# function train_GP\n",
    "class GP_predictor:\n",
    "    # Constructor\n",
    "    def __init__(self, \n",
    "                 input_set, \n",
    "                 observation_set, \n",
    "                 GPkernel = 1 * RBF(length_scale=1.0, length_scale_bounds=(1e-1, 1e2)), \n",
    "                 n_optimizers = 9):\n",
    "        # Scale input data\n",
    "        self.input_set = [list(x) for x in input_set]\n",
    "        self.observation_set = [list(x) for x in observation_set]\n",
    "        self.input_dimension = len(self.input_set[0])\n",
    "        self.observation_dimension = len(self.observation_set[0])\n",
    "        self.trainset_length = len(self.input_set)\n",
    "        \n",
    "        self.input_scaler = preprocessing.StandardScaler()\n",
    "        self.input_scaler.fit(np.array(self.input_set))\n",
    "        \n",
    "        # Scale output data\n",
    "        self.observation_scaler = preprocessing.StandardScaler()\n",
    "        self.observation_scaler.fit(np.array(observation_set))\n",
    "        \n",
    "        # Fit Gaussian process\n",
    "        self.GP = GaussianProcessRegressor(kernel = GPkernel, n_restarts_optimizer = n_optimizers)\n",
    "        self.GP.fit(self.input_scaler.transform(np.array(self.input_set)), \n",
    "                    self.observation_scaler.transform(np.array(self.observation_set)))\n",
    "        \n",
    "    # Predict on a new input set\n",
    "    def predict(self, new_input_set):\n",
    "        # Predict new observation\n",
    "        new_observation = self.observation_scaler.inverse_transform(\n",
    "            self.GP.predict(\n",
    "                self.input_scaler.transform(np.array(list(new_input_set)).reshape([-1, self.input_dimension]))\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        return new_observation\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a set, run cases, get observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myGP = GP_predictor(input_set, )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
