{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import h5 reader\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "work_path = \"/home/shengduo/pylith-developer/build/debug/pylith-nonRegSlipLawWithVaryingB/examples/bar_shearwave/quad4/\"\n",
    "h5_path = work_path + \"output/fault/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class RunABatch\n",
    "class RunABatch:\n",
    "    # Constructor\n",
    "    def __init__(self, input_set, work_path, FourierTerms = 16):\n",
    "        # Initialize data paths and batch parameters\n",
    "        self.work_path = work_path\n",
    "        self.input_set = input_set\n",
    "        self.h5_path = work_path + \"output/fault/\"\n",
    "        self.FourierTerms = FourierTerms\n",
    "        \n",
    "        # Store all cases in the h5_path\n",
    "        self.existingCasesFile = self.h5_path + \"CaseList.csv\"\n",
    "        \n",
    "        # Flag for whether cases have been run\n",
    "        self.casesExcuted = False\n",
    "        \n",
    "        # Get existing cases\n",
    "        self.getExistingCasesOfInputSet()\n",
    "        \n",
    "        # Get input_set_toRun\n",
    "        self.input_set_toRun = list(set(input_set) - self.existingCases)\n",
    "        \n",
    "        # Run cases\n",
    "        self.runCases(self.input_set_toRun)\n",
    "        \n",
    "        # Get obsevations\n",
    "        self.Observations = self.getObservations(self.input_set)\n",
    "    \n",
    "    # Inline function gets [A, B] list\n",
    "    def getAAndB(self, fileName):\n",
    "        A_idx = fileName.find('A')\n",
    "        B_idx = fileName.find('B')\n",
    "        slash_idx = fileName.find('-')\n",
    "        \n",
    "        # Change this part !! Before applying to new name convention\n",
    "        A = float(fileName[A_idx + 1 : B_idx - 1])\n",
    "        B = float(fileName[B_idx + 1 : slash_idx])\n",
    "        return (A, B)\n",
    "    \n",
    "    \n",
    "    # Function get_existing_files_set\n",
    "    def getExistingCasesOfInputSet(self):\n",
    "        # Get all .h5 file names as a list\n",
    "        myPath = self.h5_path\n",
    "        onlyFiles = [f for f in listdir(myPath) if (isfile(join(myPath, f)) and f[-3 : ] == '.h5')]\n",
    "        self.existingCases = set([self.getAAndB(f) for f in onlyFiles])\n",
    "        \n",
    "    # Function runCases\n",
    "    def runCases(self, input_set):\n",
    "        shell_path = self.work_path + \"RunJobsJP.sh\"\n",
    "        shellRead = open(shell_path, 'r')\n",
    "        list_of_lines = shellRead.readlines()\n",
    "        shellRead.close()\n",
    "\n",
    "        AA = [ele[0] for ele in input_set]\n",
    "        BB = [ele[1] for ele in input_set]\n",
    "\n",
    "        list_of_lines[9] = \"AA=\" + str(tuple(AA)).replace(',', '') + \"\\n\"\n",
    "        list_of_lines[10] = \"BB=\" + str(tuple(BB)).replace(',', '') + \"\\n\"\n",
    "\n",
    "        shellWrite = open(shell_path, 'w')\n",
    "        shellWrite.writelines(list_of_lines)\n",
    "        shellWrite.close()\n",
    "\n",
    "        # Run the cases\n",
    "        !source $shell_path\n",
    "        \n",
    "        self.casesExcuted = True\n",
    "        \n",
    "        # Return from shell\n",
    "        return\n",
    "    \n",
    "    \n",
    "    # Function getObservations for the input_set\n",
    "    def getObservations(self, input_set):\n",
    "        # Initialize Observations\n",
    "        Observations = []\n",
    "        \n",
    "        # Check if the cases have been excuted\n",
    "        if self.casesExcuted == False:\n",
    "            return Observations\n",
    "        \n",
    "        # Loop through all Inputs\n",
    "        for input_ele in input_set:\n",
    "            # Open the file\n",
    "            h5_file = self.h5_path + \"A\" + str(input_ele[0]) + \"_B\" + str(input_ele[1]) + \"-fault.h5\"\n",
    "            f = h5py.File(h5_file, 'r')\n",
    "\n",
    "            # Get time\n",
    "            time = np.array(f['time']).reshape([-1])\n",
    "            time = time - np.min(time)\n",
    "            nOfTSteps = time.shape[0]\n",
    "\n",
    "            # Get Slip rates\n",
    "            SlipRates = np.array(f['vertex_fields']['slip_rate'])\n",
    "            Vx = SlipRates[:, :, 0].transpose()\n",
    "            Vy = SlipRates[:, :, 1].transpose()\n",
    "            nOfNodes = Vx.shape[0]\n",
    "\n",
    "            # Find the Fourier coefficients\n",
    "            FourierTerms = self.FourierTerms\n",
    "            T = np.max(time)\n",
    "\n",
    "            # Compute the Fourier terms\n",
    "            Ks = np.array(range(FourierTerms))\n",
    "            coskPiTt = np.cos(Ks.reshape([-1, 1]) * np.pi / T * time)\n",
    "            VxcoskPiTt = np.concatenate([coskPiTt * Vxi.reshape([1, -1]) for Vxi in Vx], 0)\n",
    "\n",
    "            # Compute the fourier coefficients\n",
    "            # print('time.shape: ', time.shape)\n",
    "            observation = np.trapz(VxcoskPiTt, x=time)\n",
    "\n",
    "            # Append the result from this file\n",
    "            Observations.append(observation)\n",
    "        \n",
    "        Observations = np.array(Observations)\n",
    "        return Observations\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/shengduo/InverseProblems/GPRWorkingField\r\n"
     ]
    }
   ],
   "source": [
    "# Input example of getting observations\n",
    "AA = np.linspace(0.002, 0.014, 7)\n",
    "BB = AA + 0.004\n",
    "\n",
    "# Get input set\n",
    "input_set = set([(AA[i], BB[i]) for i in range(len(AA))])\n",
    "\n",
    "# Test class RunABatch\n",
    "myBatch = RunABatch(input_set, work_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.004, 0.008],\n",
       "       [0.014, 0.018],\n",
       "       [0.006, 0.01 ],\n",
       "       [0.01 , 0.014],\n",
       "       [0.002, 0.006],\n",
       "       [0.008, 0.012],\n",
       "       [0.012, 0.016]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(list(myBatch.input_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the sampling packages\n",
    "import emcee\n",
    "\n",
    "# Set up a GP\n",
    "myGP = GP_predictor(myBatch.input_set, myBatch.Observations)\n",
    "\n",
    "# Define the log probability function\n",
    "def log_prob(u, u_low, u_high, y, GP, si_eta = 1.):\n",
    "    normal_idx = np.all(np.concatenate([u >= u_low, u <= u_high], axis = 1), axis = 1)\n",
    "    res = - 0.5 / si_eta ** 2 * np.sum((y - GP.predict(u)) ** 2, axis = 1)\n",
    "    res[~normal_idx] = -np.inf\n",
    "    return res\n",
    "\n",
    "# Use a sample y\n",
    "y = myBatch.Observations[0]\n",
    "\n",
    "# Sample from p(u|y)\n",
    "uLow = np.array([0.002, 0.006])\n",
    "uLength = np.array([0.01, 0.01])\n",
    "uHigh = uLow + uLength\n",
    "\n",
    "ndim, nwalkers = 2, 100\n",
    "p0 = np.random.uniform(size = [nwalkers, ndim]) * uLength + uSt\n",
    "\n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, log_prob, args=[uLow, uHigh, y, myGP], vectorize = True)\n",
    "sampler.run_mcmc(p0, 1000)\n",
    "\n",
    "samples = sampler.get_last_sample().coords\n",
    "samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.random.uniform(size=[5, 2])\n",
    "len(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class BayersianInv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Bayersian Inv that solves a Bayersian inversion problem\n",
    "class BayersianInv:\n",
    "    # Constructor\n",
    "    def __init__(self, u_low, u_high, y, work_path, atol = 1.0e-4, n_samples = 20, MCMCsteps = 1000):\n",
    "        self.u_low = u_low\n",
    "        self.u_high = u_high\n",
    "        self.input_dim = len(u_low)\n",
    "        \n",
    "        self.y = y\n",
    "        self.output_dim = len(y)\n",
    "        self.work_path = work_path\n",
    "        self.atol = atol\n",
    "        \n",
    "        self.n_samples = n_samples\n",
    "        self.MCMCsteps = MCMCsteps\n",
    "        \n",
    "        self.GPs = []\n",
    "        self.U = np.empty([0, self.input_dim])\n",
    "        self.Observations = np.empty([0, self.output_dim])\n",
    "        self.iterations = 0\n",
    "    \n",
    "    # Get the accumulative probability function\n",
    "    def log_prob(u, y, si_eta = 1.):\n",
    "        # Apply hard constraints\n",
    "        normal_idx = np.all(np.concatenate([u >= self.u_low, u <= self.u_high], axis = 1), axis = 1)\n",
    "        \n",
    "        # First prior is uniform distribution\n",
    "        res = np.ones(len(u))\n",
    "        res[~normal_idx] = -np.inf\n",
    "        \n",
    "        # Add all posteriors in each iteration\n",
    "        for i in range(self.iterations):\n",
    "            res[normal_idx] += -0.5 / si_eta ** 2 * (\n",
    "                np.sum(\n",
    "                    (y - self.GPs[i].predict(u[normal_idx])) ** 2, \n",
    "                    axis = 1\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        # Return the log_probability at the current iteration\n",
    "        return res\n",
    "    \n",
    "    # Run one iteration\n",
    "    def runOneIteration(n_samples):\n",
    "        # Sample from the current distribution for u and get the current observations\n",
    "        sampler = emcee.EnsembleSampler(n_samples, \n",
    "                                        self.input_dim, \n",
    "                                        self.log_prob, args=[y], \n",
    "                                        vectorize = True)\n",
    "        \n",
    "        # Initialize uniformly as the starting point\n",
    "        p0 = np.random.uniform(size = [n_samples, self.input_dim]) * (self.u_high - self.u_low) + self.u_low\n",
    "        \n",
    "        # Get the result\n",
    "        sampler.run_mcmc(p0, self.MCMCsteps)\n",
    "        self.samples = sampler.get_last_sample().coords\n",
    "        \n",
    "        # Check if the standard deviation of the samples is smaller than atol\n",
    "        if np.std(self.samples) < self.atol:\n",
    "            # Converging flag True\n",
    "            return True\n",
    "        \n",
    "        else:\n",
    "            # Run the forward map with the samples\n",
    "            myBatch = RunABatch(samples, self.work_path)\n",
    "            self.U = np.append(self.U, samples, axis = 0)\n",
    "            self.O = np.append(self.O, myBatch.Observations, axis = 0)\n",
    "\n",
    "            # Get a new Gaussian process\n",
    "            myGP = GP_predictor(self.U, self.O)\n",
    "            self.GPs.append(myGP)\n",
    "            self.iterations += 1\n",
    "            \n",
    "            # Converging flag False\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = sampler.get_last_sample().coords\n",
    "samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.64600015, 0.40125205],\n",
       "       [0.4025001 , 0.93104859],\n",
       "       [0.76505991, 0.47248174],\n",
       "       [0.54590508, 0.8524087 ],\n",
       "       [0.2984607 , 0.41978312]])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "O = np.empty([0, 2])\n",
    "a = np.random.uniform(size = [5, 2])\n",
    "O = np.append(O, a, axis = 0)\n",
    "O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.041407644915402e-01"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.64600015, 0.40125205],\n",
       "       [0.4025001 , 0.93104859],\n",
       "       [0.76505991, 0.47248174],\n",
       "       [0.54590508, 0.8524087 ],\n",
       "       [0.2984607 , 0.41978312]])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.uniform()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Gaussian-regression related functions\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "\n",
    "# Pre-process the data\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# function train_GP\n",
    "class GP_predictor:\n",
    "    # Constructor\n",
    "    def __init__(self, \n",
    "                 input_set, \n",
    "                 observation_set, \n",
    "                 GPkernel = 1 * RBF(length_scale=1.0, length_scale_bounds=(1e-1, 1e2)), \n",
    "                 n_optimizers = 9):\n",
    "        # Scale input data\n",
    "        self.input_set = [list(x) for x in input_set]\n",
    "        self.observation_set = [list(x) for x in observation_set]\n",
    "        self.input_dimension = len(self.input_set[0])\n",
    "        self.observation_dimension = len(self.observation_set[0])\n",
    "        self.trainset_length = len(self.input_set)\n",
    "        \n",
    "        self.input_scaler = preprocessing.StandardScaler()\n",
    "        self.input_scaler.fit(np.array(self.input_set))\n",
    "        \n",
    "        # Scale output data\n",
    "        self.observation_scaler = preprocessing.StandardScaler()\n",
    "        self.observation_scaler.fit(np.array(observation_set))\n",
    "        \n",
    "        # Fit Gaussian process\n",
    "        self.GP = GaussianProcessRegressor(kernel = GPkernel, n_restarts_optimizer = n_optimizers)\n",
    "        self.GP.fit(self.input_scaler.transform(np.array(self.input_set)), \n",
    "                    self.observation_scaler.transform(np.array(self.observation_set)))\n",
    "        \n",
    "    # Predict on a new input set\n",
    "    def predict(self, new_input_set):\n",
    "        # Predict new observation\n",
    "        new_observation = self.observation_scaler.inverse_transform(\n",
    "            self.GP.predict(\n",
    "                self.input_scaler.transform(np.array(list(new_input_set)).reshape([-1, self.input_dimension]))\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        return new_observation\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a set, run cases, get observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myGP = GP_predictor(input_set, )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
