{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import h5 reader\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "work_path = \"/home/shengduo/pylith-developer/build/debug/pylith-nonRegSlipLawWithVaryingB/examples/bar_shearwave/quad4/\"\n",
    "h5_path = work_path + \"output/fault/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class RunABatch\n",
    "class RunABatch:\n",
    "    # Constructor\n",
    "    def __init__(self, input_set, work_path, FourierTerms = 16):\n",
    "        # Initialize data paths and batch parameters\n",
    "        self.work_path = work_path\n",
    "        self.input_set = input_set\n",
    "        self.h5_path = work_path + \"output/fault/\"\n",
    "        self.FourierTerms = FourierTerms\n",
    "        \n",
    "        # Store all cases in the h5_path\n",
    "        self.existingCasesFile = self.h5_path + \"CaseList.csv\"\n",
    "        \n",
    "        # Flag for whether cases have been run\n",
    "        self.casesExcuted = False\n",
    "        \n",
    "        # Get existing cases\n",
    "        self.getExistingCasesOfInputSet()\n",
    "        \n",
    "        # Get input_set_toRun\n",
    "        self.input_set_toRun = list(set(input_set) - self.existingCases)\n",
    "        \n",
    "        # Run cases\n",
    "        self.runCases(self.input_set_toRun)\n",
    "        \n",
    "        # Get obsevations\n",
    "        self.Observations = self.getObservations(self.input_set)\n",
    "    \n",
    "    # Inline function gets [A, B] list\n",
    "    def getAAndB(self, fileName):\n",
    "        A_idx = fileName.find('A')\n",
    "        B_idx = fileName.find('B')\n",
    "        slash_idx = fileName.find('-')\n",
    "        \n",
    "        # Change this part !! Before applying to new name convention\n",
    "        A = float(fileName[A_idx + 1 : B_idx - 1])\n",
    "        B = float(fileName[B_idx + 1 : slash_idx])\n",
    "        return (A, B)\n",
    "    \n",
    "    \n",
    "    # Function get_existing_files_set\n",
    "    def getExistingCasesOfInputSet(self):\n",
    "        # Get all .h5 file names as a list\n",
    "        myPath = self.h5_path\n",
    "        onlyFiles = [f for f in listdir(myPath) if (isfile(join(myPath, f)) and f[-3 : ] == '.h5')]\n",
    "        self.existingCases = set([self.getAAndB(f) for f in onlyFiles])\n",
    "        \n",
    "    # Function runCases\n",
    "    def runCases(self, input_set):\n",
    "        shell_path = self.work_path + \"RunJobsJP.sh\"\n",
    "        shellRead = open(shell_path, 'r')\n",
    "        list_of_lines = shellRead.readlines()\n",
    "        shellRead.close()\n",
    "\n",
    "        AA = [ele[0] for ele in input_set]\n",
    "        BB = [ele[1] for ele in input_set]\n",
    "\n",
    "        list_of_lines[9] = \"AA=\" + str(tuple(AA)).replace(',', '') + \"\\n\"\n",
    "        list_of_lines[10] = \"BB=\" + str(tuple(BB)).replace(',', '') + \"\\n\"\n",
    "\n",
    "        shellWrite = open(shell_path, 'w')\n",
    "        shellWrite.writelines(list_of_lines)\n",
    "        shellWrite.close()\n",
    "\n",
    "        # Run the cases\n",
    "        !source $shell_path\n",
    "        \n",
    "        self.casesExcuted = True\n",
    "        \n",
    "        # Return from shell\n",
    "        return\n",
    "    \n",
    "    \n",
    "    # Function getObservations for the input_set\n",
    "    def getObservations(self, input_set):\n",
    "        # Initialize Observations\n",
    "        Observations = []\n",
    "        \n",
    "        # Check if the cases have been excuted\n",
    "        if self.casesExcuted == False:\n",
    "            return Observations\n",
    "        \n",
    "        # Loop through all Inputs\n",
    "        for input_ele in input_set:\n",
    "            # Open the file\n",
    "            h5_file = self.h5_path + \"A\" + str(input_ele[0]) + \"_B\" + str(input_ele[1]) + \"-fault.h5\"\n",
    "            f = h5py.File(h5_file, 'r')\n",
    "\n",
    "            # Get time\n",
    "            time = np.array(f['time']).reshape([-1])\n",
    "            time = time - np.min(time)\n",
    "            nOfTSteps = time.shape[0]\n",
    "\n",
    "            # Get Slip rates\n",
    "            SlipRates = np.array(f['vertex_fields']['slip_rate'])\n",
    "            Vx = SlipRates[:, :, 0].transpose()\n",
    "            Vy = SlipRates[:, :, 1].transpose()\n",
    "            nOfNodes = Vx.shape[0]\n",
    "\n",
    "            # Find the Fourier coefficients\n",
    "            FourierTerms = self.FourierTerms\n",
    "            T = np.max(time)\n",
    "\n",
    "            # Compute the Fourier terms\n",
    "            Ks = np.array(range(FourierTerms))\n",
    "            coskPiTt = np.cos(Ks.reshape([-1, 1]) * np.pi / T * time)\n",
    "            VxcoskPiTt = np.concatenate([coskPiTt * Vxi.reshape([1, -1]) for Vxi in Vx], 0)\n",
    "\n",
    "            # Compute the fourier coefficients\n",
    "            # print('time.shape: ', time.shape)\n",
    "            observation = np.trapz(VxcoskPiTt, x=time)\n",
    "\n",
    "            # Append the result from this file\n",
    "            Observations.append(observation)\n",
    "            \n",
    "        return Observations\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running case A0.014_B0.018000000000000002\n",
      "Finished in 12 s!\n",
      "\n",
      "/home/shengduo/InverseProblems/GPRWorkingField\n"
     ]
    }
   ],
   "source": [
    "# Input example of getting observations\n",
    "AA = np.linspace(0.002, 0.014, 7)\n",
    "BB = AA + 0.004\n",
    "\n",
    "# Get input set\n",
    "input_set = set([(AA[i], BB[i]) for i in range(len(AA))])\n",
    "\n",
    "# Test class RunABatch\n",
    "myBatch = RunABatch(input_set, work_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'emcee'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-106-a25f0aa47ad7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Import the sampling packages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0memcee\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Set up a GP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmyGP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGP_predictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyBatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmyBatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mObservations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'emcee'"
     ]
    }
   ],
   "source": [
    "# Import the sampling packages\n",
    "import emcee\n",
    "\n",
    "# Set up a GP\n",
    "myGP = GP_predictor(myBatch.input_set, myBatch.Observations)\n",
    "\n",
    "# Define the log probability function\n",
    "def log_prob(u, y, GP, si_eta = 1.):\n",
    "    return - 0.5 / si_eta ** 2 * np.sum((y - GP.predict(u)) ** 2)\n",
    "\n",
    "# Use a sample y\n",
    "y = myBatch.Observations[0]\n",
    "\n",
    "# Sample from p(u|y)\n",
    "uSt = np.array([0.002, 0.006])\n",
    "uLength = np.array([0.01, 0.01])\n",
    "\n",
    "ndim, nwalkers = 2, 100\n",
    "p0 = np.random.uniform(size = [nwalkers, ndim]) * uLength + uSt\n",
    "\n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, log_prob, args=[y, myGp])\n",
    "sampler.run_mcmc(p0, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.uniform()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Gaussian-regression related functions\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "\n",
    "# Pre-process the data\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# function train_GP\n",
    "class GP_predictor:\n",
    "    # Constructor\n",
    "    def __init__(self, \n",
    "                 input_set, \n",
    "                 observation_set, \n",
    "                 GPkernel = 1 * RBF(length_scale=1.0, length_scale_bounds=(1e-1, 1e2)), \n",
    "                 n_optimizers = 9):\n",
    "        # Scale input data\n",
    "        self.input_scaler = preprocessing.StandardScaler()\n",
    "        self.input_scaler.fit(np.array(input_set))\n",
    "        \n",
    "        # Scale output data\n",
    "        self.observation_scaler = preprocessing.StandardScaler()\n",
    "        self.observation_scaler.fit(np.array(observation_set))\n",
    "        \n",
    "        # Fit Gaussian process\n",
    "        self.GP = GaussianProcessRegressor(kernel = GPkernel, n_restarts_optimizer = n_optimizers)\n",
    "        self.GP.fit(self.input_scaler.transform(np.array(input_set)), \n",
    "                    self.observation_scaler.transform(np.array(observation_set)))\n",
    "        \n",
    "    # Predict on a new input set\n",
    "    def predict(self, new_input_set):\n",
    "        # Predict new observation\n",
    "        new_observation = self.observation_scaler.inverse_transform(\n",
    "            self.GP.predict(\n",
    "                self.input_scaler.transform(np.array(new_input_set))\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        return new_observation\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a set, run cases, get observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myGP = GP_predictor(input_set, )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
