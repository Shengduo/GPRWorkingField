{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of workers available:  36\n"
     ]
    }
   ],
   "source": [
    "# import h5 reader\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "import matplotlib\n",
    "import scipy.optimize as opt\n",
    "from matplotlib.ticker import LinearLocator\n",
    "from scipy.interpolate import griddata, interp1d\n",
    "import emcee\n",
    "from joblib import Parallel, delayed, effective_n_jobs\n",
    "import imageio\n",
    "matplotlib.rcParams['mathtext.fontset'] = 'stix'\n",
    "matplotlib.rcParams['font.family'] = 'STIXGeneral'\n",
    "\n",
    "work_path = \"/home/shengduo/pylith-developer/build/debug/pylith-nonRegSlipLawWithVaryingB/examples/2d/InverseUniExp/\"\n",
    "h5_path = work_path + \"output/fault/\"\n",
    "print(\"Number of workers available: \", effective_n_jobs(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string(val):\n",
    "    return \"{:.16e}\".format(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Gaussian-regression related functions\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "\n",
    "# Pre-process the data\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# function train_GP\n",
    "class GP_predictor:\n",
    "    # Constructor\n",
    "    def __init__(self, \n",
    "                 input_set, \n",
    "                 observation_set, \n",
    "                 GPkernel = 1 * RBF(length_scale=1.0, length_scale_bounds=(1e-3, 1e3)), \n",
    "                 n_optimizers = 9):\n",
    "        # Scale input data\n",
    "        self.input_set = [list(x) for x in input_set]\n",
    "        self.observation_set = [list(x) for x in observation_set]\n",
    "        self.input_dimension = len(self.input_set[0])\n",
    "        self.observation_dimension = len(self.observation_set[0])\n",
    "        self.trainset_length = len(self.input_set)\n",
    "        \n",
    "        self.input_scaler = preprocessing.StandardScaler()\n",
    "        self.input_scaler.fit(np.array(self.input_set))\n",
    "        \n",
    "        # Scale output data\n",
    "        self.observation_scaler = preprocessing.StandardScaler()\n",
    "        self.observation_scaler.fit(np.array(observation_set))\n",
    "        \n",
    "        # Fit Gaussian process\n",
    "        self.GP = GaussianProcessRegressor(kernel = GPkernel, n_restarts_optimizer = n_optimizers)\n",
    "        # self.GP = MyGPR(kernel = GPkernel, n_restarts_optimizer = n_optimizers, max_iter = max_iterations)\n",
    "        self.GP.fit(self.input_scaler.transform(np.array(self.input_set)), \n",
    "                    self.observation_scaler.transform(np.array(self.observation_set)))\n",
    "        \n",
    "    # Predict on a new input set\n",
    "    def predict(self, new_input_set):\n",
    "        # Predict new observation\n",
    "        new_observation = self.observation_scaler.inverse_transform(\n",
    "            self.GP.predict(\n",
    "                self.input_scaler.transform(np.array(list(new_input_set)).reshape([-1, self.input_dimension]))\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        return new_observation\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class RunABatch\n",
    "class RunABatch:\n",
    "    # Constructor\n",
    "    def __init__(self, input_set, work_path, FourierTerms = 16, distanceAbove = 2e-3, \n",
    "                 nOfQueryPts = 10, obsFlag = 'fault', fourierFlag = False):\n",
    "        # Initialize data paths and batch parameters\n",
    "        self.work_path = work_path\n",
    "        self.input_set = [tuple(i) for i in input_set]\n",
    "        self.h5_path = work_path + \"output/faultFiles/\"\n",
    "        self.frontsurf_path = work_path + \"output/frontsurfFiles/\"\n",
    "        self.FourierTerms = FourierTerms\n",
    "        self.distanceAbove = distanceAbove\n",
    "        self.nOfQueryPts = nOfQueryPts\n",
    "        self.fourierFlag = fourierFlag\n",
    "        \n",
    "        # Store all cases in the h5_path\n",
    "        self.existingCasesFile = self.h5_path + \"CaseList.csv\"\n",
    "        \n",
    "        # Flag for whether cases have been run\n",
    "        self.casesExcuted = False\n",
    "        \n",
    "        # Get existing cases\n",
    "        self.getExistingCasesOfInputSet()\n",
    "        \n",
    "        # Get input_set_toRun\n",
    "        self.input_set_toRun = list(set(self.input_set) - self.existingCases)\n",
    "        \n",
    "        # Run cases\n",
    "        self.runCases(self.input_set_toRun)\n",
    "        \n",
    "        # Get obsevations\n",
    "        # self.Observations = self.getObservations(self.input_set)\n",
    "        if obsFlag == 'fault':\n",
    "            self.Observations = self.getObservations(self.input_set, self.nOfQueryPts)\n",
    "        elif obsFlag == 'front':\n",
    "            self.Observations = self.getObservationsFrontSurf(self.input_set, self.nOfQueryPts, self.distanceAbove)\n",
    "        else:\n",
    "            self.Observations = self.getObservationsEveryWhere(self.input_set)\n",
    "            \n",
    "    # Inline function gets [A, B] list\n",
    "    def getABDrsThetaFstar(self, fileName):\n",
    "        A_idx = fileName.find('A')\n",
    "        B_idx = fileName.find('B')\n",
    "        Drs_idx = fileName.find('D')\n",
    "        Theta_idx = fileName.find('t')\n",
    "        Fstar_idx = fileName.find('f')\n",
    "        slash_idx = fileName.find('-fault')\n",
    "\n",
    "        # Change this part !! Before applying to new name convention\n",
    "        # print(\"fileName: \", fileName)\n",
    "        A = float(fileName[A_idx + 1 : B_idx - 1])\n",
    "        B = float(fileName[B_idx + 1 : Drs_idx - 1])\n",
    "        Drs = float(fileName[Drs_idx + 3 : Theta_idx - 1])\n",
    "        Theta = float(fileName[Theta_idx + 5 : Fstar_idx - 1])\n",
    "        Fstar = float(fileName[Fstar_idx + 5 : slash_idx])\n",
    "        return (A, B, Drs, Theta, Fstar)\n",
    "    \n",
    "    \n",
    "    # Function get_existing_files_set\n",
    "    def getExistingCasesOfInputSet(self):\n",
    "        # Get all .h5 file names as a list\n",
    "        myPath = self.h5_path\n",
    "        onlyFiles = [f for f in listdir(myPath) if (isfile(join(myPath, f)) and f[-8 : ] == 'fault.h5')]\n",
    "        self.existingCases = set([self.getABDrsThetaFstar(f) for f in onlyFiles])\n",
    "        \n",
    "    # Function runCases\n",
    "    def runCases(self, input_set, nThreads=24):\n",
    "        # Start the timer\n",
    "        st_time = time.time()\n",
    "        \n",
    "        # Edit the directories\n",
    "        shell_pathR = self.work_path + \"RunJobsJP.sh\"\n",
    "        shellRead = open(shell_pathR, 'r')\n",
    "        list_of_lines = shellRead.readlines()\n",
    "        shellRead.close()\n",
    "\n",
    "        # Divide the input_set into nThreads subsets\n",
    "        if nThreads > len(input_set):\n",
    "            nRealThreads = len(input_set)\n",
    "        else:\n",
    "            nRealThreads = nThreads\n",
    "        \n",
    "        if nRealThreads > 0:\n",
    "            # Split with Real thread numbers\n",
    "            split_input_set = np.array_split(input_set, nRealThreads)\n",
    "            split_threadNo = list(range(1, nRealThreads + 1))\n",
    "            \n",
    "#             # Print splits\n",
    "#             for inputSet, tNo in zip(split_input_set, split_threadNo):\n",
    "#                 print(inputSet, tNo)\n",
    "            \n",
    "            # Submit the jobs\n",
    "            time_thread = Parallel(n_jobs=nRealThreads, backend='threading')(\n",
    "                delayed(self.runCasesOneThread)(list_of_lines, inputSet, tNo) for inputSet, tNo in zip(split_input_set, split_threadNo)\n",
    "            )\n",
    "        else:\n",
    "            time_thread = [0.]\n",
    "        \n",
    "        # Get running time\n",
    "        time_cost = time.time() - st_time\n",
    "        \n",
    "        # Print running time\n",
    "        # print(\"Wall clock time to run all inputs: \", time_cost)\n",
    "        # print(\"Sum of all thread times: \", sum(time_thread))\n",
    "        \n",
    "        # Return from shell\n",
    "        self.casesExcuted = True\n",
    "        \n",
    "        return\n",
    "    \n",
    "    # Function runCasesOneThread\n",
    "    def runCasesOneThread(self, list_of_lines, input_set, threadNo):\n",
    "        # Start timer\n",
    "        st_time = time.time()\n",
    "        \n",
    "        # Deep copy the list\n",
    "        write_lines = list_of_lines.copy()\n",
    "        \n",
    "        # Set output path\n",
    "        shell_pathW = self.work_path + \"RunJobsJP-\" + str(threadNo) + \".sh\"\n",
    "        \n",
    "        # Extract parameters\n",
    "        AA = [string(ele[0]) for ele in input_set]\n",
    "        BB = [string(ele[1]) for ele in input_set]\n",
    "        DD = [string(ele[2]) for ele in input_set]\n",
    "        TT = [string(ele[3]) for ele in input_set]\n",
    "        FF = [string(ele[4]) for ele in input_set]\n",
    "        \n",
    "        # Change the lines\n",
    "        write_lines[9] = \"AA=\" + str(tuple(AA)).replace(',', '').replace('\\'', '') + \"\\n\"\n",
    "        write_lines[10] = \"BB=\" + str(tuple(BB)).replace(',', '').replace('\\'', '') + \"\\n\"\n",
    "        write_lines[11] = \"DD=\" + str(tuple(DD)).replace(',', '').replace('\\'', '') + \"\\n\"\n",
    "        write_lines[12] = \"TT=\" + str(tuple(TT)).replace(',', '').replace('\\'', '') + \"\\n\"\n",
    "        write_lines[13] = \"FF=\" + str(tuple(FF)).replace(',', '').replace('\\'', '') + \"\\n\"\n",
    "        write_lines[14] = \"threadNo=\" + str(threadNo) + \"\\n\"\n",
    "        \n",
    "        # Write to file\n",
    "        shellWrite = open(shell_pathW, 'w')\n",
    "        shellWrite.writelines(write_lines)\n",
    "        shellWrite.close()\n",
    "        \n",
    "        #Excute the job\n",
    "        !source $shell_pathW\n",
    "        \n",
    "        # Return time\n",
    "        return time.time() - st_time\n",
    "    \n",
    "    # Function getObservations for the input_set\n",
    "    def getObservations(self, input_set, nOfQueryPts):\n",
    "        # Initialize Observations\n",
    "        Observations = []\n",
    "        \n",
    "        # Check if the cases have been excuted\n",
    "        if self.casesExcuted == False:\n",
    "            return Observations\n",
    "        \n",
    "        # Rotation matrix Q\n",
    "        alpha = 29 / 180 * np.pi\n",
    "        Q = np.array([[np.cos(alpha), np.sin(alpha)], [-np.sin(alpha), np.cos(alpha)]])\n",
    "        \n",
    "        # VS start\n",
    "        VSstart = np.array([0.006354, 0.003522])\n",
    "        \n",
    "        # Loop through all Inputs\n",
    "        for input_ele in input_set:\n",
    "            # Open the file\n",
    "            h5_file = self.h5_path + \"A\" + string(input_ele[0]) + \"_B\" + string(input_ele[1]) \\\n",
    "                      + \"_DRS\" + string(input_ele[2]) + \"_theta\" + string(input_ele[3]) + \"_fStar\" + \\\n",
    "                      string(input_ele[4]) + \"-fault.h5\"\n",
    "                     \n",
    "            f = h5py.File(h5_file, 'r')\n",
    "\n",
    "            # Get time\n",
    "            times = np.array(f['time']).reshape([-1])\n",
    "            times = times - np.min(times)\n",
    "            nOfTSteps = times.shape[0]\n",
    "            \n",
    "            # Get coordinates\n",
    "            coords = np.array(f['geometry']['vertices']) - VSstart\n",
    "            Qcoords = coords @ Q.transpose()\n",
    "            XXs = Qcoords[:, 0]\n",
    "            \n",
    "            # Get Slip rates [time, nOfNodes, spaceDim]\n",
    "            SlipRates = np.array(f['vertex_fields']['slip_rate'])\n",
    "            SlipRatesMag = np.linalg.norm(SlipRates, ord = 2, axis = 2)\n",
    "            nOfNodes = SlipRatesMag.shape[1]\n",
    "            \n",
    "            xqs = 1e-3 * np.linspace(0., 45., nOfQueryPts)\n",
    "            \n",
    "            # Store the slip rates\n",
    "            slip_rate_func = interp1d(XXs, SlipRatesMag, kind = 'cubic')  # [nOfTSteps, nOfNodes]\n",
    "            \n",
    "            # Change slip rate x to [nOfNodes, nOfTSteps]\n",
    "            slip_rate_x = slip_rate_func(xqs).transpose()\n",
    "            \n",
    "#             # DEBUG LINES\n",
    "#             print('nOfTSteps: ', nOfTSteps)\n",
    "#             print('nOfNodes: ', nOfNodes)\n",
    "#             print('SlipRatesMag shape: ', SlipRatesMag.shape)\n",
    "#             print('slip_rate_x shape: ', slip_rate_x.shape)\n",
    "            \n",
    "            # Get the observations\n",
    "            if self.fourierFlag:\n",
    "                # Find the Fourier coefficients\n",
    "                FourierTerms = self.FourierTerms\n",
    "                T = np.max(times)\n",
    "\n",
    "                # Compute the Fourier terms\n",
    "                Ks = np.array(range(FourierTerms))\n",
    "                coskPiTt = np.cos(Ks.reshape([-1, 1]) * np.pi / T * times)\n",
    "                VxcoskPiTt = np.concatenate([coskPiTt * Vxi.reshape([1, -1]) for Vxi in slip_rate_x], 0)\n",
    "\n",
    "                # Compute the fourier coefficients\n",
    "                # print('time.shape: ', time.shape)\n",
    "                observation = np.trapz(VxcoskPiTt, x=times)\n",
    "                # print(\"observation shape: \", observation.shape)\n",
    "                # Append the result from this file\n",
    "                Observations.append(observation)\n",
    "            else:\n",
    "                # Find nFourier terms of values, evenly spaced in [0, T]\n",
    "                T = np.max(times)\n",
    "                tts = np.linspace(0., T, self.FourierTerms)\n",
    "                observation_func = interp1d(times, slip_rate_x)\n",
    "                observation = observation_func(tts).reshape([-1])\n",
    "                Observations.append(observation)\n",
    "                \n",
    "        Observations = np.array(Observations)\n",
    "        return Observations\n",
    "    \n",
    "    # Function getObservationsFrontSurf for the input_set\n",
    "    def getObservationsFrontSurf(self, input_set, nOfQueryPts, distanceAbove):\n",
    "        # Initialize Observations\n",
    "        Observations = []\n",
    "        \n",
    "        # Rotation matrix Q\n",
    "        alpha = 29 / 180 * np.pi\n",
    "        Q = np.array([[np.cos(alpha), np.sin(alpha)], [-np.sin(alpha), np.cos(alpha)]])\n",
    "        \n",
    "        # VS start\n",
    "        VSstart = np.array([0.006354, 0.003522])\n",
    "        \n",
    "        # Query points\n",
    "        distance_above = distanceAbove  # Distance above the interface\n",
    "        \n",
    "        # Set x_up query points\n",
    "        x_up = 1e-3 * np.linspace(0., 45., nOfQueryPts)\n",
    "        QueryPts_up = np.stack([x_up, distance_above * np.ones(x_up.shape)], axis = 1)\n",
    "        QueryPts_dn = np.stack([x_up, -distance_above * np.ones(x_up.shape)], axis = 1)\n",
    "        \n",
    "        # nOfNodes\n",
    "        nOfNodes = len(x_up)\n",
    "            \n",
    "        # Check if the cases have been excuted\n",
    "        if self.casesExcuted == False:\n",
    "            return Observations\n",
    "        \n",
    "        # Loop through all Inputs\n",
    "        for input_ele in input_set:\n",
    "            # Open the file\n",
    "            h5_file = self.frontsurf_path + \"A\" + string(input_ele[0]) + \"_B\" + string(input_ele[1]) \\\n",
    "                      + \"_DRS\" + string(input_ele[2]) + \"_theta\" + string(input_ele[3]) + \"_fStar\" + \\\n",
    "                      string(input_ele[4]) + \"-domain.h5\"\n",
    "                     \n",
    "            f = h5py.File(h5_file, 'r')\n",
    "            \n",
    "            # Get time\n",
    "            times = np.array(f['time']).reshape([-1])\n",
    "            times = times - np.min(times)\n",
    "            nOfTSteps = times.shape[0]\n",
    "\n",
    "            # Get coordinates\n",
    "            coords = np.array(f['geometry']['vertices']) - VSstart\n",
    "            Qcoords = coords @ Q.transpose()\n",
    "            \n",
    "            # Store the slip rates\n",
    "            slip_rate_x = np.zeros([nOfTSteps, nOfNodes])\n",
    "            \n",
    "            # Get Slip rates\n",
    "            velocity = np.array(f['vertex_fields']['velocity'])\n",
    "            Qvelocity = velocity @ Q.transpose()\n",
    "            \n",
    "            for i in range(nOfTSteps):\n",
    "                slip_rate_x[i, :] = - griddata(Qcoords, velocity[i, :, 0], QueryPts_up, method = 'cubic') \\\n",
    "                              + griddata(Qcoords, velocity[i, :, 0], QueryPts_dn, method = 'cubic')\n",
    "            slip_rate_x = slip_rate_x.transpose()\n",
    "            \n",
    "            # Get the observations\n",
    "            if self.fourierFlag:\n",
    "                # Find the Fourier coefficients\n",
    "                FourierTerms = self.FourierTerms\n",
    "                T = np.max(times)\n",
    "\n",
    "                # Compute the Fourier terms\n",
    "                Ks = np.array(range(FourierTerms))\n",
    "                coskPiTt = np.cos(Ks.reshape([-1, 1]) * np.pi / T * times)\n",
    "                VxcoskPiTt = np.concatenate([coskPiTt * Vxi.reshape([1, -1]) for Vxi in slip_rate_x], 0)\n",
    "\n",
    "                # Compute the fourier coefficients\n",
    "                # print('time.shape: ', time.shape)\n",
    "                observation = np.trapz(VxcoskPiTt, x=times)\n",
    "                # print(\"observation shape: \", observation.shape)\n",
    "                # Append the result from this file\n",
    "                Observations.append(observation)\n",
    "            \n",
    "            else:\n",
    "                # Find nFourier terms of values, evenly spaced in [0, T]\n",
    "                T = np.max(times)\n",
    "                tts = np.linspace(0., T, self.FourierTerms)\n",
    "                observation_func = interp1d(times, slip_rate_x)\n",
    "                observation = observation_func(tts).reshape([-1])\n",
    "                Observations.append(observation)\n",
    "        \n",
    "        Observations = np.array(Observations)\n",
    "        return Observations\n",
    "    \n",
    "    # Function getObservationsFrontSurf for the input_set\n",
    "    def getObservationsEveryWhere(self, input_set):\n",
    "        # Initialize Observations\n",
    "        Observations = []\n",
    "        \n",
    "        # Rotation matrix Q\n",
    "        alpha = 29 / 180 * np.pi\n",
    "        Q = np.array([[np.cos(alpha), np.sin(alpha)], [-np.sin(alpha), np.cos(alpha)]])\n",
    "        \n",
    "        # VS start\n",
    "        VSstart = np.array([0.006354, 0.003522])\n",
    "            \n",
    "        # Check if the cases have been excuted\n",
    "        if self.casesExcuted == False:\n",
    "            return Observations\n",
    "        \n",
    "        # Loop through all Inputs\n",
    "        for input_ele in input_set:\n",
    "            # Open the file\n",
    "            h5_file = self.frontsurf_path + \"A\" + string(input_ele[0]) + \"_B\" + string(input_ele[1]) \\\n",
    "                      + \"_DRS\" + string(input_ele[2]) + \"_theta\" + string(input_ele[3]) + \"_fStar\" + \\\n",
    "                      string(input_ele[4]) + \"-domain.h5\"\n",
    "                     \n",
    "            f = h5py.File(h5_file, 'r')\n",
    "            \n",
    "            # Get time\n",
    "            times = np.array(f['time']).reshape([-1])\n",
    "            times = times - np.min(times)\n",
    "            nOfTSteps = len(times)\n",
    "            \n",
    "            # Get coordinates\n",
    "            coords = np.array(f['geometry']['vertices']) - VSstart\n",
    "            Qcoords = coords @ Q.transpose()\n",
    "            \n",
    "            # Get the node list in the observable zone\n",
    "            nodeList = np.all(\n",
    "                np.stack(\n",
    "                    [Qcoords[:, 0] >= 0., Qcoords[:, 0] <= 46.e-3, Qcoords[:, 1] >= -15.e-3, Qcoords[:, 1] <= 15.e-3]\n",
    "                ), \n",
    "                axis = 0 \n",
    "            )\n",
    "            \n",
    "            # Get Slip rates\n",
    "            velocity = np.array(f['vertex_fields']['velocity'])[:, nodeList, :]\n",
    "            Qvelocity = velocity @ Q.transpose()\n",
    "            # print(\"Qvelocity shape: \", Qvelocity.shape)\n",
    "            # Collapse all measurements\n",
    "            slip_rate_x = Qvelocity.reshape([Qvelocity.shape[0], -1]).transpose()\n",
    "            \n",
    "            \n",
    "            \n",
    "            # Find nFourier terms of values, evenly spaced in [0, T]\n",
    "            T = np.max(times)\n",
    "            tts = np.linspace(0., T, len(times) // 5)\n",
    "            observation_func = interp1d(times, slip_rate_x)\n",
    "            observation = observation_func(tts).reshape([-1])\n",
    "            Observations.append(observation)\n",
    "        \n",
    "        Observations = np.array(Observations)\n",
    "        return Observations\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.rand([5])\n",
    "b = np.rand([3])\n",
    "a.append(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class BayersianInv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split a hyper-rectangle into n_gridPts^K rectangles\n",
    "def divideRects(low_hi, nGridPts):\n",
    "    # Provide low, high values of any given rectangle, evenly divide into nGridPts\n",
    "    low = low_hi[0]\n",
    "    hi = low_hi[1]\n",
    "    newLows = np.linspace(low, hi, nGridPts)[0 : -1].transpose()\n",
    "    delta = (hi - low) / (nGridPts - 1)\n",
    "    \n",
    "    # Change newLows into starting points\n",
    "    shits = np.meshgrid(*newLows)\n",
    "    newLows = np.array([x.reshape(-1) for x in shits]).transpose()\n",
    "    newHighs = newLows + delta\n",
    "    areas = [np.product(delta) for i in range(len(newLows))]\n",
    "    \n",
    "    # Get all the  grid points\n",
    "    AllGridPts = np.linspace(low, hi, nGridPts).transpose()\n",
    "    \n",
    "    # Change newLows into starting points\n",
    "    shits = np.meshgrid(*AllGridPts)\n",
    "    AllGridPts = np.array([x.reshape(-1) for x in shits]).transpose()\n",
    "    AllGridPts = AllGridPts.reshape([-1, len(low)])\n",
    "    \n",
    "    return [[newLows[i], newHighs[i]] for i in range(len(newLows))], areas, AllGridPts\n",
    "\n",
    "# Find the smallest rectangle that contains this point\n",
    "# Rects and Rects_area has to be lists\n",
    "def findSmallestRectContainsPt(Rects, Rects_area, pt):\n",
    "    # Order the rectangles by area\n",
    "    ord_idx = np.argsort(Rects_area)\n",
    "    Rects = [Rects[i] for i in ord_idx]\n",
    "    Rects_area = [Rects_area[i] for i in ord_idx]\n",
    "    \n",
    "#     # DEBUG LINES\n",
    "#     print(\"Rects: \", Rects)\n",
    "#     print(\"Rects_area: \", Rects_area)\n",
    "    \n",
    "    \n",
    "    # Find the rectangle that pt is in\n",
    "    for idx in range(len(Rects)):\n",
    "        rect = Rects[idx]\n",
    "#         # DEBUG LINES\n",
    "#         print(\"idx: \", idx)\n",
    "#         print(\"rect: \", rect)\n",
    "#         print(\"rect[0]: \", rect[0])\n",
    "#         print(\"rect[1]: \", rect[1])\n",
    "#         print(\"pt: \", pt)\n",
    "#         print(\"(np.all(rect[0] <= pt): \", (np.all(rect[0] <= pt)))\n",
    "#         print(\"(np.all(rect[1] >= pt): \", (np.all(rect[1] >= pt)))\n",
    "        \n",
    "        if (np.all(rect[0] <= pt) and np.all(rect[1] >= pt)):\n",
    "            res_idx = idx\n",
    "            res_rect = rect\n",
    "            break\n",
    "    \n",
    "    return res_idx, res_rect\n",
    "\n",
    "# Substitute Rectangles in the list with a new set of rectangles\n",
    "def substituteRects(Rects, Rects_area, pt, nGridPts):\n",
    "    # Divide the target rectangle\n",
    "    idx, rect = findSmallestRectContainsPt(Rects, Rects_area, pt)\n",
    "    Rect = Rects.pop(idx)\n",
    "    RectArea = Rects_area.pop(idx)\n",
    "    newRects, newAreas, AllGridPts = divideRects(Rect, nGridPts)\n",
    "    \n",
    "    # DEBUG LINES\n",
    "    print(\"Rect and nGridPts: \", Rect, nGridPts)\n",
    "    \n",
    "    # Append the new rectangles and new areas\n",
    "    Rects = Rects.append(newRects)\n",
    "    Rects_area = Rects_area.append(newAreas)\n",
    "    \n",
    "    # Print Rects\n",
    "    print(\"Rects: \", Rects)\n",
    "    print(\"Rects_area: \", Rects_area)\n",
    "    \n",
    "    # Return all new points\n",
    "    return AllGridPts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[array([0.005, 0.01 ]), array([0.015, 0.02 ])]]"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in 154 s!\n",
      "\n",
      "Running case A8.8297872340425531e-03_B1.5744680851063831e-02_DRS3.0000000000000001e-06_theta6.0000000000000001e-03_fStar5.7999999999999996e-01 on thread 10\n",
      "Finished in 154 s!\n",
      "\n",
      "Finished in 156 s!\n",
      "\n",
      "Running case A1.0319148936170211e-02_B1.2127659574468085e-02_DRS3.0000000000000001e-06_theta6.0000000000000001e-03_fStar5.7999999999999996e-01 on thread 9\n",
      "Running case A1.4999999999999999e-02_B1.4255319148936171e-02_DRS3.0000000000000001e-06_theta6.0000000000000001e-03_fStar5.7999999999999996e-01 on thread 6\n",
      "Finished in 155 s!\n",
      "\n",
      "Finished in 155 s!\n",
      "\n",
      "Running case A5.6382978723404260e-03_B1.8297872340425531e-02_DRS3.0000000000000001e-06_theta6.0000000000000001e-03_fStar5.7999999999999996e-01 on thread 19\n",
      "Running case A1.3723404255319149e-02_B1.3617021276595745e-02_DRS3.0000000000000001e-06_theta6.0000000000000001e-03_fStar5.7999999999999996e-01 on thread 7\n",
      "Finished in 153 s!\n",
      "\n",
      "Running case A9.8936170212765955e-03_B1.5744680851063831e-02_DRS3.0000000000000001e-06_theta6.0000000000000001e-03_fStar5.7999999999999996e-01 on thread 8\n",
      "Finished in 156 s!\n",
      "\n",
      "Running case A1.0531914893617021e-02_B1.4680851063829787e-02_DRS3.0000000000000001e-06_theta6.0000000000000001e-03_fStar5.7999999999999996e-01 on thread 5\n",
      "Finished in 156 s!\n",
      "\n",
      "Finished in 154 s!\n",
      "\n",
      "Running case A1.4361702127659574e-02_B1.9361702127659575e-02_DRS3.0000000000000001e-06_theta6.0000000000000001e-03_fStar5.7999999999999996e-01 on thread 3\n",
      "Running case A1.1595744680851063e-02_B1.0425531914893617e-02_DRS3.0000000000000001e-06_theta6.0000000000000001e-03_fStar5.7999999999999996e-01 on thread 2\n",
      "Finished in 154 s!\n",
      "\n",
      "Finished in 153 s!\n",
      "\n",
      "Finished in 154 s!\n",
      "\n",
      "Finished in 155 s!\n",
      "\n",
      "Finished in 154 s!\n",
      "\n",
      "Running case A1.2872340425531915e-02_B1.0000000000000000e-02_DRS3.0000000000000001e-06_theta6.0000000000000001e-03_fStar5.7999999999999996e-01 on thread 22\n",
      "Finished in 154 s!\n",
      "\n",
      "Running case A6.0638297872340425e-03_B1.4893617021276596e-02_DRS3.0000000000000001e-06_theta6.0000000000000001e-03_fStar5.7999999999999996e-01 on thread 4\n",
      "Running case A1.4787234042553190e-02_B1.9361702127659575e-02_DRS3.0000000000000001e-06_theta6.0000000000000001e-03_fStar5.7999999999999996e-01 on thread 17\n",
      "Running case A7.3404255319148935e-03_B1.6595744680851066e-02_DRS3.0000000000000001e-06_theta6.0000000000000001e-03_fStar5.7999999999999996e-01 on thread 15\n",
      "Finished in 154 s!\n",
      "\n",
      "Finished in 154 s!\n",
      "\n",
      "Running case A1.1382978723404255e-02_B1.5957446808510641e-02_DRS3.0000000000000001e-06_theta6.0000000000000001e-03_fStar5.7999999999999996e-01 on thread 20\n",
      "Running case A1.0319148936170211e-02_B1.1489361702127660e-02_DRS3.0000000000000001e-06_theta6.0000000000000001e-03_fStar5.7999999999999996e-01 on thread 11\n",
      "Running case A7.7659574468085107e-03_B1.4468085106382979e-02_DRS3.0000000000000001e-06_theta6.0000000000000001e-03_fStar5.7999999999999996e-01 on thread 13\n",
      "Running case A1.1808510638297871e-02_B1.8936170212765960e-02_DRS3.0000000000000001e-06_theta6.0000000000000001e-03_fStar5.7999999999999996e-01 on thread 18\n",
      "Finished in 155 s!\n",
      "\n",
      "Finished in 155 s!\n",
      "\n",
      "Running case A5.2127659574468087e-03_B1.5531914893617021e-02_DRS3.0000000000000001e-06_theta6.0000000000000001e-03_fStar5.7999999999999996e-01 on thread 24\n"
     ]
    }
   ],
   "source": [
    "myInv.Rects = [[np.array(myInv.u_low), np.array(myInv.u_high)]] \n",
    "myInv.Rects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myInv.RectsArea = [np.product(myInv.u_high - myInvf.u_low)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [199]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43msubstituteRects\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmyInv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmyInv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRectsArea\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmyInv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaxLikelihoodUs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [198]\u001b[0m, in \u001b[0;36msubstituteRects\u001b[0;34m(Rects, Rects_area, pt, nGridPts)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msubstituteRects\u001b[39m(Rects, Rects_area, pt, nGridPts):\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# Divide the target rectangle\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m     idx, rect \u001b[38;5;241m=\u001b[39m \u001b[43mfindSmallestRectContainsPt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mRects_area\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     Rect \u001b[38;5;241m=\u001b[39m Rects\u001b[38;5;241m.\u001b[39mpop(idx)\n\u001b[1;32m     62\u001b[0m     RectArea \u001b[38;5;241m=\u001b[39m Rects_area\u001b[38;5;241m.\u001b[39mpop(idx)\n",
      "Input \u001b[0;32mIn [198]\u001b[0m, in \u001b[0;36mfindSmallestRectContainsPt\u001b[0;34m(Rects, Rects_area, pt)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfindSmallestRectContainsPt\u001b[39m(Rects, Rects_area, pt):\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# Order the rectangles by area\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     ord_idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort(Rects_area)\n\u001b[0;32m---> 30\u001b[0m     Rects \u001b[38;5;241m=\u001b[39m [Rects[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m ord_idx]\n\u001b[1;32m     31\u001b[0m     Rects_area \u001b[38;5;241m=\u001b[39m [Rects_area[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m ord_idx]\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m#     # DEBUG LINES\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m#     print(\"Rects: \", Rects)\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m#     print(\"Rects_area: \", Rects_area)\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     \n\u001b[1;32m     37\u001b[0m     \n\u001b[1;32m     38\u001b[0m     \u001b[38;5;66;03m# Find the rectangle that pt is in\u001b[39;00m\n",
      "Input \u001b[0;32mIn [198]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfindSmallestRectContainsPt\u001b[39m(Rects, Rects_area, pt):\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# Order the rectangles by area\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     ord_idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort(Rects_area)\n\u001b[0;32m---> 30\u001b[0m     Rects \u001b[38;5;241m=\u001b[39m [\u001b[43mRects\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m ord_idx]\n\u001b[1;32m     31\u001b[0m     Rects_area \u001b[38;5;241m=\u001b[39m [Rects_area[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m ord_idx]\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m#     # DEBUG LINES\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m#     print(\"Rects: \", Rects)\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m#     print(\"Rects_area: \", Rects_area)\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     \n\u001b[1;32m     37\u001b[0m     \n\u001b[1;32m     38\u001b[0m     \u001b[38;5;66;03m# Find the rectangle that pt is in\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "# \n",
    "substituteRects(myInv.Rects, myInv.RectsArea, myInv.maxLikelihoodUs[-1], 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Bayersian Inv that solves a Bayersian inversion problem\n",
    "class BayersianInv:\n",
    "    # Constructor\n",
    "    def __init__(self, u_low, u_high, u, y, work_path, FourierTerms = 16, \n",
    "                 atol = 1.0e-6, si_eta = 0.5, n_samples = 20, MCMCsteps = 1000, save_path = \"./\", \n",
    "                 distanceAbove = 2e-3, nOfQueryPts = 10, obsFlag = 'fault', fourierFlag = False, \n",
    "                 samplesFlag = 'sampling'):\n",
    "        # Set X has to be compact in R^k\n",
    "        self.real_u_idx = np.array((u_low != u_high))\n",
    "        self.u_low = np.array(u_low[self.real_u_idx])\n",
    "        self.u_high = np.array(u_high[self.real_u_idx])\n",
    "        self.dummy_u = np.array(u_low[~self.real_u_idx])\n",
    "        \n",
    "        \n",
    "        # For each iteration, needs a region to build the grid points\n",
    "        self.u_lows = [u_low]\n",
    "        self.u_highs = [u_high]\n",
    "        \n",
    "        # Store all the Rectangles\n",
    "        self.Rects = [[np.array(self.u_low), np.array(self.u_high)]] \n",
    "        self.RectsArea = [np.product(self.u_high - self.u_low)]\n",
    "        \n",
    "        # The true value of u, as well as the observation y to be inverted\n",
    "        self.u = np.array(u)\n",
    "        self.y = np.array(y)\n",
    "        \n",
    "        # Other parameters\n",
    "        self.input_dim_All = len(u_low)\n",
    "        self.input_dim = len(self.u_low)\n",
    "        self.output_dim = len(self.y)\n",
    "        self.work_path = work_path\n",
    "        self.FourierTerms = FourierTerms\n",
    "        self.atol = atol\n",
    "        self.si_eta = si_eta\n",
    "        self.n_samples = n_samples\n",
    "        self.MCMCsteps = MCMCsteps\n",
    "        self.save_path = save_path\n",
    "        \n",
    "        # Keep record of distanceAbove and nOfQueryPts in the field of view\n",
    "        self.distanceAbove = distanceAbove\n",
    "        self.nOfQueryPts = nOfQueryPts\n",
    "        \n",
    "        # Get where the observations are from\n",
    "        self.obsFlag = obsFlag\n",
    "        self.fourierFlag = fourierFlag\n",
    "        self.samplesFlag = samplesFlag\n",
    "        \n",
    "        # Keep records of the sampled points and the corresponding observations after iteration\n",
    "        self.U = np.empty([0, self.input_dim])\n",
    "        self.O = np.empty([0, self.output_dim])\n",
    "        self.iterations = 0\n",
    "        \n",
    "        # Keep record of eta, MaxMinLenRatio, GaussianProcess Emulator, average error on the sampled points, \n",
    "        # and empirical mean and stdv of the posterior after each iteration\n",
    "        self.etas = []\n",
    "        self.maxMinDistRatio = []\n",
    "        self.GPs = []\n",
    "        self.avg_errors = []\n",
    "        self.mean = []\n",
    "        self.stdv = []\n",
    "        \n",
    "        # Maximum likelihood points\n",
    "        self.maxLikelihoodUs = [(self.u_low + self.u_high) / 2]\n",
    "        self.maxLikelihoodUsPropL2Error = []\n",
    "        self.maxLikelihoodObsPropL2Error = []\n",
    "    \n",
    "    # Cast u into the same length of u_low, with dummy u\n",
    "    def castU(self, u_real):\n",
    "        # print(\"u_real shape: \", u_real.shape)\n",
    "        u_real_this = u_real.reshape([-1, self.real_u_idx.sum()])\n",
    "        # print(\"u_real_this shape: \", u_real_this.shape)\n",
    "        \n",
    "        res = np.zeros([u_real_this.shape[0], len(self.u)])\n",
    "        res[:, self.real_u_idx] = u_real_this\n",
    "        res[:, ~self.real_u_idx] = self.dummy_u\n",
    "        return res\n",
    "    \n",
    "    # Get the accumulative probability function\n",
    "    def log_prob(self, u, y):\n",
    "        # Apply hard constraints\n",
    "        u_reshape = u.reshape([-1, self.input_dim])\n",
    "        normal_idx = np.all(\n",
    "            np.concatenate(\n",
    "                [u_reshape >= self.u_low, u_reshape <= self.u_high], axis = 1\n",
    "            ), \n",
    "            axis = 1\n",
    "        )\n",
    "       \n",
    "        # First prior is uniform distribution\n",
    "        res = np.ones(len(u_reshape))\n",
    "        res[~normal_idx] = -np.inf\n",
    "        \n",
    "        # Add all posteriors in each iteration (since this is log of probability density)\n",
    "        if (np.sum(normal_idx) > 0):\n",
    "            for i in range(self.iterations):\n",
    "                res[normal_idx] += -0.5 / self.etas[i] ** 2 * (\n",
    "                    np.sum(\n",
    "                        (y - self.GPs[i].predict(u_reshape[normal_idx])) ** 2, \n",
    "                        axis = 1\n",
    "                    )\n",
    "                )\n",
    "        \n",
    "        # Return the log_probability at the current iteration\n",
    "        return res\n",
    "    \n",
    "    # Compute statistics: sample and calculate mean and stdv\n",
    "    def compute_stats(self, n_samples, n_steps = 1_000):\n",
    "#         # Sample for more points to update the empirical statistics\n",
    "#         sampler = emcee.EnsembleSampler(n_samples, \n",
    "#                                         self.input_dim, \n",
    "#                                         self.log_prob, args=[y], \n",
    "#                                         vectorize = True)\n",
    "\n",
    "#         # Initialize uniformly as the starting point\n",
    "#         p0 = np.random.uniform(size = [n_samples, self.input_dim]) * (self.u_high - self.u_low) + self.u_low\n",
    "\n",
    "#         # Get the result\n",
    "#         sampler.run_mcmc(p0, n_steps)\n",
    "#         samples = sampler.get_last_sample().coords\n",
    "#         self.mean.append(np.mean(samples, axis = 0))\n",
    "#         self.stdv.append(np.std(samples, axis = 0))\n",
    "        \n",
    "        # Get maximum likelihood estimate\n",
    "        fun = lambda u: -self.log_prob(u, self.y)\n",
    "        newCenter = opt.minimize(fun, x0 = np.mean(self.samples, axis = 0), \n",
    "                                 bounds = [(self.u_low[i], self.u_high[i]) for i in range(len(self.u_low))]\n",
    "                                ).x\n",
    "        \n",
    "        self.maxLikelihoodUs.append(newCenter)\n",
    "        \n",
    "        self.maxLikelihoodUsPropL2Error.append(\n",
    "            np.linalg.norm(self.u[self.real_u_idx] - self.maxLikelihoodUs[-1]) \n",
    "            / np.linalg.norm(self.u[self.real_u_idx])\n",
    "        )\n",
    "        self.maxLikelihoodObsPropL2Error.append(\n",
    "            np.linalg.norm(self.y - self.GPs[-1].predict(self.maxLikelihoodUs[-1])) \n",
    "            / np.linalg.norm(self.y)\n",
    "        )\n",
    "        \n",
    "        # Find the new rectangle\n",
    "        \n",
    "        \n",
    "#         # Function to determine if a point is in the rectangles\n",
    "#         def isInRectangle(setOfRects, pt):\n",
    "#             res = [ for rec in setOfRects]\n",
    "        \n",
    "    # Get average error of a batch\n",
    "    def get_avg_error_of_a_batch(self, myBatch):\n",
    "        return np.mean(np.sum((myBatch.Observations - self.y) ** 2, axis = 1))\n",
    "    \n",
    "    # Run one iteration\n",
    "    def runOneIteration(self, n_samples, n_stat_samples):\n",
    "        # Switch based on samplesFlag\n",
    "        if self.samplesFlag == 'sampling':\n",
    "            # Sample from the current distribution for u and get the current observations\n",
    "            sampler = emcee.EnsembleSampler(n_samples, \n",
    "                                            self.input_dim, \n",
    "                                            self.log_prob, args=[y], \n",
    "                                            vectorize = True)\n",
    "\n",
    "            # Initialize uniformly as the starting point\n",
    "            p0 = np.random.uniform(size = [n_samples, self.input_dim]) * (self.u_high - self.u_low) + self.u_low\n",
    "\n",
    "            # Get the result\n",
    "            sampler.run_mcmc(p0, self.MCMCsteps)\n",
    "            self.samples = sampler.get_last_sample().coords\n",
    "\n",
    "            # If there is self_etas, this is to make sure MaxMinDistRatio does not go too large\n",
    "            if len(self.etas) > 0:\n",
    "                eta_times = 0\n",
    "                while (eta_times < 2) and (MaxMinDistRatio(self.samples) > 100.):\n",
    "                    # Double the last eta\n",
    "                    self.etas[-1] = 2. * self.etas[-1]\n",
    "                    eta_times += 1\n",
    "\n",
    "                    # Re-sample\n",
    "                    sampler = emcee.EnsembleSampler(n_samples, \n",
    "                                                    self.input_dim, \n",
    "                                                    self.log_prob, args=[y], \n",
    "                                                    vectorize = True)\n",
    "\n",
    "                    # Initialize uniformly as the starting point\n",
    "                    p0 = np.random.uniform(size = [n_samples, self.input_dim]) * (self.u_high - self.u_low) + self.u_low\n",
    "\n",
    "                    # Get the result\n",
    "                    sampler.run_mcmc(p0, self.MCMCsteps)\n",
    "                    self.samples = sampler.get_last_sample().coords\n",
    "                    MCsteps = 0\n",
    "                    while (MCsteps < self.MCMCsteps / 10) and (MaxMinDistRatio(self.samples) > 10.):\n",
    "                        sampler.run_mcmc(sampler.get_last_sample(), 1)\n",
    "                        self.samples = sampler.get_last_sample().coords\n",
    "                        MCsteps += 1\n",
    "\n",
    "                # print(\"eta_times, MaxMinDistRatio: \", eta_times, MaxMinDistRatio(self.samples))\n",
    "        \n",
    "        elif self.samplesFlag == 'grid':\n",
    "            # DEBUG LINEs\n",
    "            print(\"Before updating the rectangles\")\n",
    "            # print(\"self.samples.shape:, \", self.samples.shape)\n",
    "            print(\"self.Rects length: \", len(self.Rects))\n",
    "            print(\"self.RectsArea length: \", len(self.RectsArea))\n",
    "\n",
    "            # Start with the point being maxLikelihoodUs[-1]\n",
    "            self.samples = substituteRects(self.Rects, self.RectsArea, self.maxLikelihoodUs[-1], n_samples)\n",
    "            \n",
    "            # DEBUG LINEs\n",
    "            print(\"After updating the rectangles\")\n",
    "            print(\"self.samples.shape:, \", self.samples.shape)\n",
    "            print(\"self.Rects length: \", len(self.Rects))\n",
    "            print(\"self.RectsArea length: \", len(self.RectsArea))\n",
    "\n",
    "        # Run the forward map with the samples\n",
    "        myBatch = RunABatch(self.castU(self.samples), self.work_path, FourierTerms = self.FourierTerms, \n",
    "                            distanceAbove = self.distanceAbove, nOfQueryPts = self.nOfQueryPts, \n",
    "                            obsFlag = self.obsFlag, fourierFlag = self.fourierFlag)\n",
    "        self.U = np.append(self.U, self.samples, axis = 0)\n",
    "        self.O = np.append(self.O, myBatch.Observations, axis = 0)\n",
    "\n",
    "        # Get a new Gaussian process\n",
    "        myGP = GP_predictor(self.samples, myBatch.Observations)\n",
    "        # myGP = GP_predictor(self.U, self.O)\n",
    "\n",
    "        # Update the recorded variables of the process\n",
    "        self.GPs.append(myGP)\n",
    "        self.iterations += 1\n",
    "        self.etas.append(self.si_eta)\n",
    "        self.avg_errors.append(self.get_avg_error_of_a_batch(myBatch))\n",
    "        self.maxMinDistRatio.append(MaxMinDistRatio(self.samples))\n",
    "        \n",
    "        # Update stats\n",
    "        self.compute_stats(n_stat_samples)\n",
    "        \n",
    "        # Converging flag False\n",
    "        return False\n",
    "    \n",
    "    # Run function\n",
    "    def run(self, n_iter_max = 10, n_stat_samples = 1000):\n",
    "        for i in range(n_iter_max):\n",
    "            # In each iteration\n",
    "            print(\"=\"*30, \" Iteration \", str(self.iterations), \" \", \"=\"*30)\n",
    "            \n",
    "            # Run one iteration\n",
    "            start_time = time.time()\n",
    "            converged = self.runOneIteration(self.n_samples, n_stat_samples)\n",
    "            end_time = time.time()\n",
    "            print(\"Time cost: \", str(end_time - start_time), \" s\")\n",
    "            # print(\"Average error in this iteration: \", self.avg_errors[-1])\n",
    "            print(\"Maximum to minimum distance in the sample points: \", self.maxMinDistRatio[-1])\n",
    "            # print(\"Mean of the posterior after this iteration: \", self.mean[-1])\n",
    "            # print(\"Stdv of the posterior after this iteration: \", self.stdv[-1])\n",
    "            print(\"Max likelihood estimate of the posterior after this iteration: \", self.maxLikelihoodUs[-1])\n",
    "            print(\"L2 error of u: \", self.maxLikelihoodUsPropL2Error[-1])\n",
    "            print(\"L2 error of y: \", self.maxLikelihoodObsPropL2Error[-1])\n",
    "        # Save the current model to files\n",
    "        self.save()\n",
    "        \n",
    "    # Save the model to files\n",
    "    def save(self):\n",
    "        savePath = self.save_path + \"models/A{0}_B{1}_DRS{2}_theta{3}_fStar{4}_nOfQPts{5}_eta{6}-{7}-Fourier-{8}-samples-{9}.pickle\".format(self.u[0], self.u[1], self.u[2], self.u[3], self.u[4], self.nOfQueryPts, self.si_eta, self.obsFlag, self.fourierFlag, self.samplesFlag)\n",
    "        with open(savePath, 'wb') as file:\n",
    "            pickle.dump(self, file)\n",
    "    \n",
    "    # Plot the regression process into a gif file\n",
    "    def plotGIF(self):\n",
    "        plotSeries(self, self.u[self.real_u_idx], self.y, self.save_path)\n",
    "\n",
    "        \n",
    "# ===================================== Other Auxiliary functions ==========================================\n",
    "# Function MaxMinDistRatio, compute the ratio of maximum to minimum distance among pts, \n",
    "# for the gaussian process to converge with a reasonable length-scaled kernel, \n",
    "# this value should not be too large\n",
    "def MaxMinDistRatio(pts):\n",
    "    matrix = [[np.linalg.norm(pts[i] - pts[j]) for j in range(i + 1, len(pts))] for i in range(len(pts))]\n",
    "    vec = [x for y in matrix for x in y]\n",
    "    return max(vec) / min(vec)\n",
    "\n",
    "\n",
    "# Calculate the corresponding (to y) log likelihood of iter_step on u\n",
    "def log_prob_best(self, iter_step, u, y):\n",
    "    # Apply hard constraints\n",
    "    u_reshape = u.reshape([-1, self.input_dim])\n",
    "    normal_idx = np.all(np.concatenate([u_reshape >= self.u_low, u_reshape <= self.u_high], axis = 1), axis = 1)\n",
    "\n",
    "    # First prior is uniform distribution\n",
    "    res = np.ones(len(u_reshape))\n",
    "    res[~normal_idx] = -np.inf\n",
    "\n",
    "    # Add all posteriors in each iteration\n",
    "    if (np.sum(normal_idx) > 0):\n",
    "        for i in range(iter_step + 1):\n",
    "            res[normal_idx] += -0.5 / self.etas[i] ** 2 * (\n",
    "                np.sum(\n",
    "                    (y - self.GPs[i].predict(u_reshape[normal_idx])) ** 2, \n",
    "                    axis = 1\n",
    "                )\n",
    "            )\n",
    "\n",
    "    # Return the log_probability at the current iteration\n",
    "    return res\n",
    "\n",
    "# Plot the given regression process on (analytical_u, analytical_y) and save the GIF\n",
    "def plotSeries(self, analytical_u, analytical_y, save_path, dpi_value = 300):\n",
    "    # Calculate U_plot\n",
    "    minU = self.u_low\n",
    "    maxU = self.u_high\n",
    "    nOfGridPoints = 100\n",
    "\n",
    "    xis = []\n",
    "    for i in range(minU.shape[0]):\n",
    "        xis.append(np.linspace(minU[i], maxU[i], nOfGridPoints))\n",
    "\n",
    "    # Generate grid and draw predictions\n",
    "    UPlotGrid = np.meshgrid(xis[0], xis[1])\n",
    "    UPlotGrid = np.stack(UPlotGrid, axis = 2)\n",
    "    UPlotGridFat = UPlotGrid.reshape([nOfGridPoints * nOfGridPoints, maxU.shape[0]])\n",
    "    \n",
    "    gifName = save_path + \"figures/A{0}_B{1}.gif\".format(analytical_u[0], analytical_u[1])\n",
    "    \n",
    "    writer = imageio.get_writer(gifName, mode='I', duration = 1.0)\n",
    "    \n",
    "    # Maximum likelihood points\n",
    "    self.maxLikelihoodUs = []\n",
    "    self.maxLikelihoodUsPropL2Error = []\n",
    "    self.maxLikelihoodObsPropL2Error = []\n",
    "    \n",
    "    # Plot the series\n",
    "    for i in range(0, self.iterations):\n",
    "        plt.figure(figsize = (7, 6), dpi = dpi_value)\n",
    "        YPlotGridFat = log_prob_best(self, i, UPlotGridFat, analytical_y)\n",
    "        YPlotGridFat = YPlotGridFat - np.max(YPlotGridFat)\n",
    "        \n",
    "        # Get optimized u\n",
    "        idx = np.argmax(YPlotGridFat)\n",
    "        \n",
    "        # self.maxLikelihoodUs.append(UPlotGridFat[idx])\n",
    "        \n",
    "        fun = lambda u: -log_prob_best(self, i, u, analytical_y)\n",
    "        newCenter = opt.minimize(fun, x0 = UPlotGridFat[idx], \n",
    "                                 bounds = [(self.u_low[i], self.u_high[i]) for i in range(len(self.u_low))]\n",
    "                                ).x\n",
    "        \n",
    "        # DEBUG LINES\n",
    "        # print(\"=\" * 60)\n",
    "        # print(\"i, u = \", i, newCenter)\n",
    "        \n",
    "        # print(\"-fun(u_opt), log_prob_best(u_opt): \", -fun(newCenter), log_prob_best(self, i, newCenter, analytical_y))\n",
    "        # print(\"-fun(grid_opt), log_prob_best(grid_opt): \", -fun(UPlotGridFat[idx]), log_prob_best(self, i, UPlotGridFat[idx], analytical_y))\n",
    "        self.maxLikelihoodUs.append(newCenter)\n",
    "        \n",
    "        self.maxLikelihoodUsPropL2Error.append(\n",
    "            np.linalg.norm(analytical_u - self.maxLikelihoodUs[-1]) \n",
    "            / np.linalg.norm(analytical_u)\n",
    "        )\n",
    "        self.maxLikelihoodObsPropL2Error.append(\n",
    "            np.linalg.norm(analytical_y - self.GPs[i].predict(self.maxLikelihoodUs[-1])) \n",
    "            / np.linalg.norm(analytical_y)\n",
    "        )\n",
    "        \n",
    "        YPlotGrid = YPlotGridFat.reshape([nOfGridPoints, nOfGridPoints])\n",
    "        cp = plt.contourf(UPlotGrid[:, :, 0], UPlotGrid[:, :, 1], np.maximum(YPlotGrid, -5.))\n",
    "        \n",
    "        # Give the color bar\n",
    "        cbar = plt.colorbar(cp)\n",
    "        plt.clim([-5., 0.])\n",
    "        cbar.set_label('$-\\\\Phi(u)$', fontsize = 20)\n",
    "        \n",
    "        # Scatter the sample points\n",
    "        plt.scatter(self.U[ :(i + 1) * self.n_samples, 0], \n",
    "                    self.U[ :(i + 1) * self.n_samples, 1], s = 1, color = 'white')\n",
    "        plt.scatter(analytical_u[0], analytical_u[1], s = 15, color = 'red')\n",
    "        plt.xlabel('$u_1$', fontsize = 20)\n",
    "        plt.ylabel('$u_2$', fontsize = 20)\n",
    "        plt.title(\"The \" + str(i) + \" th iteration\")\n",
    "        figName = save_path + \"figures/shit\" + str(i) + \".png\"\n",
    "        plt.savefig(figName, dpi = dpi_value)\n",
    "        \n",
    "        # Save the figures into a gif\n",
    "        image = imageio.imread(figName)\n",
    "        writer.append_data(image)\n",
    "        !rm $figName\n",
    "    writer.close()\n",
    "    \n",
    "    # Save a figure of L2PropError of u\n",
    "    plt.figure(figsize = (7, 6), dpi = dpi_value)\n",
    "    plt.scatter(range(1, 1 + len(self.maxLikelihoodUsPropL2Error)), self.maxLikelihoodUsPropL2Error, s = 20)\n",
    "    plt.xlabel('Iteration', fontsize = 20)\n",
    "    plt.ylabel('$\\|u-u_{true}\\|/\\|u_{true}\\|$', fontsize = 20)\n",
    "    # plt.ylim([0., 0.01])\n",
    "    plt.title('Relative L2 error of $u$', fontsize = 20)\n",
    "    figName = save_path + \"figures/A{0}_B{1}_propL2UError.png\".format(analytical_u[0], analytical_u[1])\n",
    "    plt.savefig(figName, dpi = dpi_value)\n",
    "\n",
    "    # Save a figure of L2PropError of y\n",
    "    plt.figure(figsize = (7, 6), dpi = dpi_value)\n",
    "    plt.scatter(range(1, 1 + len(self.maxLikelihoodUsPropL2Error)), self.maxLikelihoodObsPropL2Error, s = 20)\n",
    "    plt.xlabel('Iteration', fontsize = 20)\n",
    "    plt.ylabel('$\\|y-y_{true}\\|/\\|y_{true}\\|$', fontsize = 20)\n",
    "    # plt.ylim([0., 0.01])\n",
    "    plt.title('Relative L2 error of $y$', fontsize = 20)\n",
    "    figName = save_path + \"figures/A{0}_B{1}_propL2YError.png\".format(analytical_u[0], analytical_u[1])\n",
    "    plt.savefig(figName, dpi = dpi_value)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from file\n",
    "def loadInvModel(model_path):\n",
    "    with open(model_path, 'rb') as file:\n",
    "        myInv = pickle.load(file)\n",
    "    return myInv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test an inversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test run a batch\n",
    "input_set = [[0.011, 0.016, 3e-6, 0.006, 0.58], [0.012, 0.015, 2e-6, 0.003, 0.6]]\n",
    "distanceAbove = 1e-3\n",
    "nOfQueryPts = 10\n",
    "fTerms = 32\n",
    "obsFlag = 'everywhere'\n",
    "samplesFlag = 'grid'\n",
    "fourierFlag = True\n",
    "myBatch = RunABatch(input_set, work_path, fTerms, distanceAbove, nOfQueryPts, obsFlag, fourierFlag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.011, 0.016, 3e-06, 0.006, 0.58), (0.012, 0.015, 2e-06, 0.003, 0.6)]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myBatch.input_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test input set:  [(0.011, 0.016, 3e-06, 0.006, 0.58), (0.012, 0.015, 2e-06, 0.003, 0.6)]\n",
      "Work path:  /home/shengduo/pylith-developer/build/debug/pylith-nonRegSlipLawWithVaryingB/examples/2d/InverseUniExp/\n"
     ]
    }
   ],
   "source": [
    "# Generate test batch\n",
    "u_low = np.array([0.005, 0.01, 3e-6, 0.006, 0.58])\n",
    "u_high = np.array([0.015, 0.02, 3e-6, 0.006, 0.58])\n",
    "nSamples = 48\n",
    "siEta = 0.1\n",
    "print(\"Test input set: \", myBatch.input_set)\n",
    "print(\"Work path: \", work_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================  Iteration  0   ==============================\n",
      "Before updating the rectangles\n",
      "self.Rects length:  1\n",
      "self.RectsArea length:  1\n",
      "Rects:  None\n",
      "Rects_area:  None\n",
      "After updating the rectangles\n",
      "self.samples.shape:,  (2304, 2)\n",
      "self.Rects length:  1\n",
      "self.RectsArea length:  1\n",
      "Running case A1.1382978723404255e-02_B1.8936170212765960e-02_DRS3.0000000000000001e-06_theta6.0000000000000001e-03_fStar5.7999999999999996e-01 on thread 4\n",
      "Running case A1.4361702127659574e-02_B1.0212765957446808e-02_DRS3.0000000000000001e-06_theta6.0000000000000001e-03_fStar5.7999999999999996e-01 on thread 2\n",
      "Running case A1.1808510638297871e-02_B1.5957446808510641e-02_DRS3.0000000000000001e-06_theta6.0000000000000001e-03_fStar5.7999999999999996e-01 on thread 3\n",
      "Running case A1.2872340425531915e-02_B1.4468085106382979e-02_DRS3.0000000000000001e-06_theta6.0000000000000001e-03_fStar5.7999999999999996e-01 on thread 5\n",
      "Running case A7.5531914893617021e-03_B1.7234042553191491e-02_DRS3.0000000000000001e-06_theta6.0000000000000001e-03_fStar5.7999999999999996e-01 on thread 9\n",
      "Running case A1.0319148936170211e-02_B1.3617021276595745e-02_DRS3.0000000000000001e-06_theta6.0000000000000001e-03_fStar5.7999999999999996e-01 on thread 1\n",
      "Running case A9.4680851063829782e-03_B1.7021276595744681e-02_DRS3.0000000000000001e-06_theta6.0000000000000001e-03_fStar5.7999999999999996e-01 on thread 7\n",
      "Running case A1.4574468085106380e-02_B1.1063829787234043e-02_DRS3.0000000000000001e-06_theta6.0000000000000001e-03_fStar5.7999999999999996e-01 on thread 11\n",
      "Running case A6.9148936170212762e-03_B1.6382978723404256e-02_DRS3.0000000000000001e-06_theta6.0000000000000001e-03_fStar5.7999999999999996e-01 on thread 13\n",
      "Running case A1.3723404255319149e-02_B1.1702127659574468e-02_DRS3.0000000000000001e-06_theta6.0000000000000001e-03_fStar5.7999999999999996e-01 on thread 6\n",
      "Running case A8.6170212765957453e-03_B1.5531914893617021e-02_DRS3.0000000000000001e-06_theta6.0000000000000001e-03_fStar5.7999999999999996e-01 on thread 8\n",
      "Running case A1.3297872340425530e-02_B1.7446808510638297e-02_DRS3.0000000000000001e-06_theta6.0000000000000001e-03_fStar5.7999999999999996e-01 on thread 15\n",
      "Running case A9.2553191489361704e-03_B1.8723404255319147e-02_DRS3.0000000000000001e-06_theta6.0000000000000001e-03_fStar5.7999999999999996e-01 on thread 12\n",
      "Running case A1.1170212765957446e-02_B1.8723404255319147e-02_DRS3.0000000000000001e-06_theta6.0000000000000001e-03_fStar5.7999999999999996e-01 on thread 10\n",
      "Running case A1.2872340425531915e-02_B1.7021276595744681e-02_DRS3.0000000000000001e-06_theta6.0000000000000001e-03_fStar5.7999999999999996e-01 on thread 17\n",
      "Running case A9.4680851063829782e-03_B1.1702127659574468e-02_DRS3.0000000000000001e-06_theta6.0000000000000001e-03_fStar5.7999999999999996e-01 on thread 18\n",
      "Running case A1.3510638297872340e-02_B1.0638297872340425e-02_DRS3.0000000000000001e-06_theta6.0000000000000001e-03_fStar5.7999999999999996e-01 on thread 19\n",
      "Running case A8.6170212765957453e-03_B1.4042553191489362e-02_DRS3.0000000000000001e-06_theta6.0000000000000001e-03_fStar5.7999999999999996e-01 on thread 23\n",
      "Running case A7.7659574468085107e-03_B1.2978723404255319e-02_DRS3.0000000000000001e-06_theta6.0000000000000001e-03_fStar5.7999999999999996e-01 on thread 14\n",
      "Running case A1.3085106382978721e-02_B1.3617021276595745e-02_DRS3.0000000000000001e-06_theta6.0000000000000001e-03_fStar5.7999999999999996e-01 on thread 20\n",
      "Running case A5.2127659574468087e-03_B1.7234042553191491e-02_DRS3.0000000000000001e-06_theta6.0000000000000001e-03_fStar5.7999999999999996e-01 on thread 24\n",
      "Running case A1.0957446808510638e-02_B1.0000000000000000e-02_DRS3.0000000000000001e-06_theta6.0000000000000001e-03_fStar5.7999999999999996e-01 on thread 16\n",
      "Running case A9.0425531914893609e-03_B1.6808510638297872e-02_DRS3.0000000000000001e-06_theta6.0000000000000001e-03_fStar5.7999999999999996e-01 on thread 21\n",
      "Running case A5.6382978723404260e-03_B1.5744680851063831e-02_DRS3.0000000000000001e-06_theta6.0000000000000001e-03_fStar5.7999999999999996e-01 on thread 22\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [197]\u001b[0m, in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m y \u001b[38;5;241m=\u001b[39m myBatch\u001b[38;5;241m.\u001b[39mObservations[testCase_idx]\n\u001b[1;32m      5\u001b[0m myInv \u001b[38;5;241m=\u001b[39m BayersianInv(u_low, u_high, u, y, work_path, FourierTerms \u001b[38;5;241m=\u001b[39m fTerms, \n\u001b[1;32m      6\u001b[0m                      n_samples \u001b[38;5;241m=\u001b[39m nSamples, \n\u001b[1;32m      7\u001b[0m                      si_eta \u001b[38;5;241m=\u001b[39m siEta, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m                      samplesFlag \u001b[38;5;241m=\u001b[39m samplesFlag\n\u001b[1;32m     13\u001b[0m                     )\n\u001b[0;32m---> 14\u001b[0m \u001b[43mmyInv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_iter_max\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [192]\u001b[0m, in \u001b[0;36mBayersianInv.run\u001b[0;34m(self, n_iter_max, n_stat_samples)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;66;03m# Run one iteration\u001b[39;00m\n\u001b[1;32m    243\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 244\u001b[0m converged \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunOneIteration\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_stat_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTime cost: \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mstr\u001b[39m(end_time \u001b[38;5;241m-\u001b[39m start_time), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m s\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Input \u001b[0;32mIn [192]\u001b[0m, in \u001b[0;36mBayersianInv.runOneIteration\u001b[0;34m(self, n_samples, n_stat_samples)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself.RectsArea length: \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mRectsArea))\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m# Run the forward map with the samples\u001b[39;00m\n\u001b[0;32m--> 213\u001b[0m myBatch \u001b[38;5;241m=\u001b[39m \u001b[43mRunABatch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcastU\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msamples\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwork_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mFourierTerms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFourierTerms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mdistanceAbove\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistanceAbove\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnOfQueryPts\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnOfQueryPts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mobsFlag\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobsFlag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfourierFlag\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfourierFlag\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mU \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mU, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples, axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mO \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mO, myBatch\u001b[38;5;241m.\u001b[39mObservations, axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n",
      "Input \u001b[0;32mIn [92]\u001b[0m, in \u001b[0;36mRunABatch.__init__\u001b[0;34m(self, input_set, work_path, FourierTerms, distanceAbove, nOfQueryPts, obsFlag, fourierFlag)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_set_toRun \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_set) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexistingCases)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Run cases\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunCases\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_set_toRun\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Get obsevations\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# self.Observations = self.getObservations(self.input_set)\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m obsFlag \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfault\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "Input \u001b[0;32mIn [92]\u001b[0m, in \u001b[0;36mRunABatch.runCases\u001b[0;34m(self, input_set, nThreads)\u001b[0m\n\u001b[1;32m     86\u001b[0m             split_threadNo \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, nRealThreads \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m#             # Print splits\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m#             for inputSet, tNo in zip(split_input_set, split_threadNo):\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m#                 print(inputSet, tNo)\u001b[39;00m\n\u001b[1;32m     91\u001b[0m             \n\u001b[1;32m     92\u001b[0m             \u001b[38;5;66;03m# Submit the jobs\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m             time_thread \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnRealThreads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mthreading\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunCasesOneThread\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlist_of_lines\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputSet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtNo\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputSet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtNo\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_input_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit_threadNo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     97\u001b[0m             time_thread \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0.\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/joblib/parallel.py:1056\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1053\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1056\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;66;03m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   1058\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/joblib/parallel.py:935\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    933\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    934\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupports_timeout\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 935\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(\u001b[43mjob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    936\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    937\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget())\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/multiprocessing/pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 765\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    766\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mready():\n\u001b[1;32m    767\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/multiprocessing/pool.py:762\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 762\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/threading.py:574\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    572\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 574\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### Run an inversion test:\n",
    "testCase_idx = 0\n",
    "u = myBatch.input_set[testCase_idx]\n",
    "y = myBatch.Observations[testCase_idx]\n",
    "myInv = BayersianInv(u_low, u_high, u, y, work_path, FourierTerms = fTerms, \n",
    "                     n_samples = nSamples, \n",
    "                     si_eta = siEta, \n",
    "                     nOfQueryPts = nOfQueryPts, \n",
    "                     distanceAbove = distanceAbove, \n",
    "                     obsFlag = obsFlag, \n",
    "                     fourierFlag = fourierFlag, \n",
    "                     samplesFlag = samplesFlag\n",
    "                    )\n",
    "myInv.run(n_iter_max = 10)\n",
    "\n",
    "# # Save to file\n",
    "# import pickle\n",
    "\n",
    "# #save it\n",
    "# with open(f'./models/myInvA4_B6.pickle', 'wb') as file:\n",
    "#     pickle.dump(myInv, file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9.999999999999999e-05]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myInv.RectsArea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.011, 0.016])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(myInv.u)[myInv.real_u_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myInv.run(n_iter_max = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
